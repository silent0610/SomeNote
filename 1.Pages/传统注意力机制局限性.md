---
Type:
  - Page
aliases: 
Status: 
tags: 
modifiedDate: 星期一, 五月 26日 2025, 8:27:57 晚上
---
即使自注意力机制被如此广泛的应用，甚至成为了深度学习的基建之一，其仍有几个悬而未决的问题。例如在计算注意力权重时，它需要对输入序列中的每个元素与其他所有元素进行比较，以确定它们之间的关联程度。这种全局的、无差别的比较可能会导致模型可能会将注意力分配到不相关或次要的特征上，进而导致模型训练效率下降或预测精度降低。此外，尽管注意力权重提供了一定的解释，但模型的内部工作机制仍可能难以理解。而最为突出的问题是**长序列处理困难**。由于自注意力的二次方复杂度，处理长序列数据时会面临巨大的计算和存储压力，效率低下。这限制了自注意力机制在处理长文本或视频序列等任务中的应用。自注意力机制的本质是计算序列中每个位置之间的相似性，用以生成注意力权重矩阵。如果输入序列长度为 n，则需要计算 nxn 的相似性矩阵，计算复杂度为 O(n^2)。
我们可以从网络结构层面了解为什么其复杂度是 n2，如图所示（网络结构图）

Attention 可以用如下形式表示

$$
Attention(Q,K,V)=softmax(QK^⊤)V
$$

这里的 $Q∈R^{n×dk},K∈R^{m×dk},V∈R^{m×dv}$，简单起见我们就没显式地写出Attention的缩放因子了。本文我们主要关心Self Attention场景，所以为了介绍上的方便统一设 $Q,K,V∈R^{n×d}$，一般场景下都有n>d甚至n≫d（BERT base里边d=64）。

事实上，我们可以很容易的得出结论，制约Attention性能的关键因素，其实是定义里边的Softmax。$QK^T$ 这一步我们得到一个n×n的矩阵。而当用于归一化的 softmax 函数的输入是一个 nxn 矩阵时，其时间复杂度为 O（n^2）。就是这一步决定了Attention的复杂度是O(n2)；如果没有Softmax，那么就是三个矩阵连乘 $QK^TV$，而矩阵乘法是满足结合率的，所以我们可以先算 $K^TV$，得到一个d×d的矩阵，然后再用Q左乘它，由于d≪n，所以这样算大致的复杂度只是O(n)。

 也就是说，去掉Softmax的Attention的复杂度可以降到最理想的线性级别O(n)
 
1. **计算和内存开销**：
    
    - **时间复杂度**：标准的自注意力计算每对输入元素之间的相似度，因此它的计算复杂度是 O(N2)O(N^2)O(N2)，其中 NNN 是输入序列的长度。对于长序列，计算开销非常大。
    - **空间复杂度**：自注意力需要存储一个大小为 N×NN \times NN×N 的矩阵来表示各元素之间的注意力权重，因此它的空间复杂度也是 O(N2)O(N^2)O(N2)。这对于长序列或大规模数据处理来说是不可接受的。
2. **长序列处理问题**：
    
    - 对于超长文本或图像（例如文本长度上千的情况），自注意力机制无法有效处理，因为随着输入长度的增加，计算资源需求呈指数级增长。在这些场景下，模型的效率和可扩展性变得极为有限。
3. **局部依赖问题**：
    - 自注意力机制能够捕捉全局依赖关系，但在某些任务中，仅仅捕捉全局依赖可能过于复杂或多余。在实际应用中，很多任务仅关注局部信息，因此传统的自注意力计算全局关系的能力可能显得不够高效。

## 相关论文

以下是一些讨论传统注意力机制局限性的论文，您可以参考它们获取更深入的分析：

1. **"Attention is all you need" (Vaswani et al., 2017)**  
    这是提出Transformer模型的开创性论文，尽管它没有专门讨论注意力的局限性，但文中提到标准自注意力机制的计算开销问题，并提出了Transformer架构作为解决方案。  
    论文链接：[Attention is all you need](https://arxiv.org/abs/1706.03762)
    
2. **"On the difficulty of training Recurrent Neural Networks" (Bengio et al., 1994)**  
    虽然这篇论文主要讨论的是RNN的问题，但它也涉及了如何处理长序列的挑战，而自注意力机制作为RNN的替代方法，面临着相似的计算和存储挑战。  
    论文链接：On the difficulty of training Recurrent Neural Networks
    
3. **"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention" (Katharopoulos et al., 2020)**  
    这篇论文直接提出了线性注意力，并讨论了标准注意力的计算开销问题。它提出了一种通过近似计算降低注意力复杂度的方式。  
    论文链接：[Transformers are RNNs](https://arxiv.org/abs/2006.16236)
    
4. **"Linformer: Self-Attention with Linear Complexity" (Wang et al., 2020)**  
    Linformer提出了一种通过低秩近似来降低自注意力计算复杂度的技术，特别适合长序列任务。该论文详细讨论了标准自注意力机制在长序列上的效率问题，并提出了改进方案。  
    论文链接：[Linformer](https://arxiv.org/abs/2006.04768)
    
5. **"Performer: A Generalized Transformer Architecture with Linear Attention" (Choromanski et al., 2020)**  
    Performer进一步探讨了线性注意力的实现，提出通过正交随机特征的方式将注意力计算复杂度降低到O(N)，并对比了传统注意力的效率问题。  
    论文链接：[Performer](https://arxiv.org/abs/2009.14794)

关于自注意力机制时间复杂度过高的研究，以下几篇论文专门讨论了这个问题，并提出了解决方案或改进的思路：

## 1. **"Attention Is All You need" (Vaswani Et Al., 2017)**

- **问题讨论**：这篇论文是Transformer的开创性工作，提出了自注意力机制并应用于机器翻译等任务。尽管论文本身并未深入讨论自注意力时间复杂度的高昂问题，但它提到了自注意力的计算复杂度是 O(N2)O(N^2)，并且随着输入序列长度 NN 的增加，计算量急剧增加。此问题在实际应用中成为了制约模型可扩展性和效率的关键因素。
- **引用**：Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. A., Kaiser, Ł., & Polosukhin, I. (2017). Attention is All You Need. _NeurIPS_.
- 论文链接：[Attention is All You Need](https://arxiv.org/abs/1706.03762)

## 2. **"Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention" (Katharopoulos Et Al., 2020)**

- **问题讨论**：这篇论文专门探讨了自注意力的时间复杂度问题，指出标准的自注意力机制在长序列输入时的计算开销非常大，导致了其在处理长序列时的低效性。为了应对这一问题，作者提出了一种通过线性注意力的近似来降低时间复杂度的新方法，将复杂度从 O(N2)O(N^2) 降低至 O(N)O(N)。
- **解决方案**：使用了一种通过因式分解的方式对自注意力计算进行优化，从而大大提升了长序列处理的效率。
- **引用**：Katharopoulos, A., Vyas, A., Downey, D., & Wainwright, M. J. (2020). Transformers Are RNNs: Fast Autoregressive Transformers with Linear Attention. _ICLR 2020_.
- 论文链接：[Transformers Are RNNs](https://arxiv.org/abs/2006.16236)

## 3. **"Linformer: Self-Attention with Linear Complexity" (Wang Et Al., 2020)**

- **问题讨论**：Linformer提出了一个优化的自注意力机制，专门解决了标准自注意力计算复杂度过高的问题。论文指出，传统的自注意力机制在处理长序列时，其 O(N2)O(N^2) 的时间复杂度非常难以扩展。Linformer通过对注意力矩阵进行低秩近似，提出将时间复杂度降至 O(N)O(N) 的方法。
- **解决方案**：使用低秩矩阵分解将传统的自注意力机制的计算复杂度显著降低，能够在保证效果的同时处理更长的序列。
- **引用**：Wang, S., Zhai, X., Yao, L., & Liu, L. (2020). Linformer: Self-Attention with Linear Complexity. _ICL 2020_.
- 论文链接：[Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)

## 4. **"Performer: A Generalized Transformer Architecture with Linear Attention" (Choromanski Et Al., 2020)**

- **问题讨论**：Performer进一步探讨了自注意力机制的时间复杂度问题，指出标准自注意力机制的 O(N2)O(N^2) 时间复杂度会限制模型在大规模数据集和长序列上的应用。Performer提出了通过使用随机特征近似（Random Features）来实现线性复杂度的自注意力，从而避免了传统自注意力的计算瓶颈。
- **解决方案**：使用正交随机特征进行注意力计算，从而将时间复杂度降低到 O(N)O(N)，这使得模型在处理长序列时更加高效。
- **引用**：Choromanski, K., Dohan, D., Gavrilyuk, V., Kaiser, Ł., & Sutskever, I. (2020). Performer: A Generalized Transformer Architecture with Linear Attention. _ICML 2020_.
- 论文链接：[Performer](https://arxiv.org/abs/2009.14794)

## 5. **"Reformer: The Efficient Transformer" (Kitaev Et Al., 2020)**

- **问题讨论**：Reformer 论文提到，标准的自注意力计算在处理长序列时非常低效，尤其是当输入序列的长度大于 10001000 时，传统的 O(N2)O(N^2) 时间复杂度显得难以应对。Reformer通过使用局部敏感哈希（LSH）来减少计算量，并将注意力计算复杂度降低到 O(Nlog⁡N)O(N \log N)。
- **解决方案**：采用局部敏感哈希技术对注意力进行稀疏化处理，从而优化计算效率。
- **引用**：Kitaev, N., Kaiser, Ł., & Shazeer, N. (2020). Reformer: The Efficient Transformer. _ICLR 2020_.
- 论文链接：[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)

## 6. **"Longformer: The Long-Document Transformer" (Beltagy Et Al., 2020)**

- **问题讨论**：Longformer主要针对长文档处理提出了一种优化方案，指出传统自注意力机制在处理长文档时非常低效，尤其是当文档长度非常长时。Longformer提出了稀疏注意力机制，将计算复杂度从 O(N2)O(N^2) 降低为 O(N)O(N)。
- **解决方案**：Longformer通过利用滑动窗口机制和全局注意力相结合的方式来实现稀疏化注意力计算，从而降低时间复杂度。
- **引用**：Beltagy, I., Lo, K., & Cohan, A. (2020). Longformer: The Long-Document Transformer. _arXiv 2020_.
- 论文链接：[Longformer](https://arxiv.org/abs/2004.05150)

---

这些论文都明确指出了标准自注意力机制在计算上的瓶颈，特别是当输入序列长度增加时，计算和存储开销呈平方级增长。为了提高自注意力机制的可扩展性，许多研究提出了通过核方法、稀疏化、低秩近似或随机特征等方式，将时间复杂度从 O(N2)O(N^2) 降低至 O(N)O(N) 或 O(Nlog⁡N)O(N \log N)，从而使其能处理更长的序列。

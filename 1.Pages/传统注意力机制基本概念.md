---
Type:
  - Page
aliases: 
tags: 
Status: 
modifiedDate: 星期四, 六月 5日 2025, 3:05:14 下午
---
注意力机制最初在2014年由**Bahdanau等人**[]引入到序列到序列（Seq2Seq）模型中，用于改善机器翻译任务。随后，Vaswani等人在2017年提出了完全依赖于注意力机制的**Transformer模型**[]，该模型在处理序列数据时不依赖于传统的循环神经网络结构，而是通过自注意力（Self-Attention）机制捕捉序列内部的依赖关系。
![attention结构介绍](assets/Pasted%20image%2020241220101417.png#centre)

注意力机制允许模型动态地对输入数据的不同部分分配不同的处理权重。这种机制可以提高模型处理复杂数据的能力，因为它能够识别并集中资源处理那些对当前任务最重要的信息。注意力机制提高了模型对重要特征的识别能力，从而提升了模型的性能。
注意力机制被广泛应用于自然语言处理（NLP）中，显著提升了机器翻译、文本分类、问答系统中的性能。**KAM-BERT**模型，通过将知识生成的注意力图直接整合到自注意力机制中，以提高预训练语言模型的性能。**Bai 等人**，提出了一种基于多级自注意力网络和门控脉冲神经P系统的序列推荐方法，旨在解决序列推荐的长期依赖问题，并充分利用上下文信息预测和生成用户行为序列。**Sible 两人**在文章中探讨了如何通过字典和注意力掩码来改进罕见词的翻译，利用自注意力机制提高翻译模型的性能。同样的，在计算机视觉领域，注意力机制被广泛应用于图像分类、目标检测、图像分割等任务。**Chang 等人**提出了一个基于注意力机制的图像生成模型，通过使用掩码技术来生成图像，展示了注意力机制在图像生成任务中的应用。**CA-Stream**是一种基于注意力的池化方法，用于提高图像识别的可解释性。该方法通过关注图像中的关键区域来提高模型的性能和解释能力。**Guo 等人**提出了一种新的视觉注意力网络，通过模拟人类视觉注意力机制来增强卷积神经网络在图像识别任务中的表现。总而言之，注意力机制已经成为深度学习中不可或缺的一部分，展现出巨大的潜力和价值。

- **Bahdanau 的 seq2seq** Neural Machine Translation by Jointly Learning to Align and Translate 
- Attention is all your need 
- Chang 等人  Maskgit: Masked generative image transformer
- Guo 等人 Visual Attention Network
- CA-Stream: Attention-based Pooling for Interpretable Image Recognition
- **KAM-BERT** Enhancing Self-Attention with Knowledge-Assisted Attention Maps

---
Type:
  - Page
aliases: 
Status: 
tags: 
modifiedDate: 星期一, 五月 26日 2025, 8:33:56 晚上
---

## 资料

### 自注意力机制

[超详细图解Self-Attention计算过程](https://zhuanlan.zhihu.com/p/410776234)
[动图轻松理解Self-Attention(自注意力机制)思想](https://zhuanlan.zhihu.com/p/619154409)
[Self-Attention 和 Multi-Head Attention 的区别——附最通俗理解！！_multihead self attention-CSDN博客](https://blog.csdn.net/leonardotu/article/details/135886678)
$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
![|300](https://img-blog.csdnimg.cn/direct/a58b1d4f3c9a4970b34ad22413122ae5.jpeg)

### Optuna

[Optuna文档](https://optuna.readthedocs.io/zh-cn/latest/tutorial/10_key_features/003_efficient_optimization_algorithms.html#sphx-glr-tutorial-10-key-features-003-efficient-optimization-algorithms-py)
[或许是东半球最好用的超参数优化框架： Optuna 简介 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/159506313)

## 过程

[代码初步理解](代码初步理解.md)
[本科毕设数据增强记录](本科毕设数据增强记录.md)
[本科毕设数据处理理解](本科毕设数据处理理解.md)
[本科毕设模型搭建](本科毕设模型搭建.md)
[[模型训练]]

## KAR

[[训练记录]]
[本科毕设实验对比](本科毕设实验对比.md)

## 研究内容

[本科毕业设计数据集准备](本科毕业设计数据集准备.md)

## 英文翻译

[DIN翻译](DIN翻译.md)

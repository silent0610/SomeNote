---
Type:
  - Page
aliases: 
tags: 
Status: 
modifiedDate: 2025/06/11, 11:28:49
---

# 线性注意力综述参考文献

- 注意力机制最初
	- BAHDANAU D, CHO K, BENGIO Y. Neural Machine Translation by Jointly Learning to Align and Translate[J]. Cornell University - arXiv, Cornell University - arXiv, 2014.
- Vaswani
	- VASWANI A, SHAZEER N, PARMAR N, et al. Attention is All you Need[J]. Neural Information Processing Systems, Neural Information Processing Systems, 2017. ✅ 2025-01-16
- KAM-Bert
	- BAI J, WANG Y, SUN H, et al. Enhancing Self-Attention with Knowledge-Assisted Attention Maps[J]. ✅ 2025-01-16
- Bai 等人 ✅ 2025-01-16
	- BAI X, HUANG Y, PENG H, et al. Sequence recommendation using multi-level self-attention network with gated spiking neural P systems[J]. ✅ 2025-01-16
- Sible 两人 ✅ 2025-01-16
	- SIBLE K, CHIANG D. Improving Rare Word Translation With Dictionaries and Attention Masking[J].
- Chang
	- CHANG H, ZHANG H, JIANG L, et al. MaskGIT: Masked Generative Image Transformer[C/OL]//2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA. 2022. http://dx.doi.org/10.1109/cvpr52688.2022.01103. DOI: 10.1109/cvpr52688.2022.01103. ✅ 2025-01-16
- CAstream ✅ 2025-01-16
	- TORRES F, ZHANG H, SICRE R, et al. CA-Stream: Attention-based pooling for interpretable image recognition[J]. ✅ 2025-01-16
- **Guo 等人** ✅ 2025-01-16
	- GUO M H, LU C Z, LIU Z N, et al. Visual Attention Network[J]. ✅ 2025-01-16
- Katharopoulos 等人 ✅ 2025-01-16
	- KATHAROPOULOS A, VYAS A, PAPPAS N, et al. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention[J]. International Conference on Machine Learning, International Conference on Machine Learning, 2020. ✅ 2025-01-16
- Flatten Transformer ✅ 2025-01-16
	- HAN D, PAN X, HAN Y, et al. FLatten Transformer: Vision Transformer using Focused Linear Attention[J]. 2023. ✅ 2025-01-16
- Agent attention ✅ 2025-01-16
	- [1] HAN D, YE T, HAN Y, et al. Agent Attention: On the Integration of Softmax and Linear Attention[J]. 2023. ✅ 2025-01-16
- Mobile Attention ✅ 2025-01-16
	- Zhiyu Yao, Jian Wang, Haixu Wu, et al. Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers[J]. 2024 ✅ 2025-01-16
- EfficientVIT ✅ 2025-01-16
	- [1] CAI H, GAN C, HAN S. EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition[J]. 2022. ✅ 2025-01-16
- Agos ✅ 2025-01-16
	- [1] AGOSTINELLI V, CHEN L. Improving Autoregressive NLP Tasks via Modular Linearized Attention[J]. 2023. ✅ 2025-01-16
- SLAB ✅ 2025-01-16
	- [1] GUO J, CHEN X, TANG Y, et al. SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization[J]. ✅ 2025-01-16
- LinRec ✅ 2025-01-16
	- [1] LIU L, CAI L, ZHANG C, et al. LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems[J]. ✅ 2025-01-16
- Based ✅ 2025-01-16
	- [1] ARORA S, EYUBOGLU S, ZHANG M, et al. Simple linear attention language models balance the recall-throughput tradeoff[J]. ✅ 2025-01-16
- Retentive ✅ 2025-01-16
	- [1] SUN Y, DONG L, HUANG S, et al. Retentive Network: A Successor to Transformer for Large Language Models[J]. ✅ 2025-01-16
- GLA ✅ 2025-01-16
	- [1] YANG S, WANG B, SHEN Y, et al. Gated Linear Attention Transformers with Hardware-Efficient Training[J]. 2023. ✅ 2025-01-16
- Mamba ✅ 2025-01-16
	- [1] GU A, DAO T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces[J]. 2023. ✅ 2025-01-16
- Mamba2 ✅ 2025-01-16
	- [1] DAO T, GU A. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality[J]. ✅ 2025-01-16
- RWKV ✅ 2025-01-16
	- PENG B, GOLDSTEIN D, ANTHONY Q, et al. Eagle and finch: Rwkv with matrix-valued states And dynamic recurrence[A/OL]. 2024. ArXiv:2404.05892. https://arxiv.org/abs/2404.05892. ✅ 2025-01-16
- Blockwise ✅ 2025-01-16
	- [1] QIU J, MA H, LEVY O, et al. Blockwise Self-Attention for Long Document Understanding[J]. Cornell University - arXiv, Cornell University - arXiv, 2019. ✅ 2025-01-16
- Parmar ✅ 2025-01-16
	- Stand-alone self-attention in vision models ✅ 2025-01-16
- Child ✅ 2025-01-16
	- Generating long sequences With sparse transformers. ✅ 2025-01-16
- Beltagy ✅ 2025-01-16
	- Longformer: The long-document trans-former ✅ 2025-01-16
- Liu ✅ 2025-01-16
	- Generating wikipedia by summarizing long sequences ✅ 2025-01-16
- Ho ✅ 2025-01-16
	- Axial attention in Multidimensional transformers ✅ 2025-01-16
- Vyas ✅ 2025-01-16
	- FAst transformers with clusTered attention. ✅ 2025-01-16
- Wang ✅ 2025-01-16
	- GLUE: A multi-task benchmark and analysis platform for natural language unDerstanding. ✅ 2025-01-16
- Kitaev ✅ 2025-01-16
	- Reformer: The efficient transFormer. ✅ 2025-01-16
- Roy ✅ 2025-01-16
	- TOkenlearner: Adaptive space-time tokenization for videos ✅ 2025-01-16
- Tay Sparse sinkhorn Attention. ✅ 2025-01-16
- Lee Set transformer: A framework for attention-based permutation-invariant neural networks. ✅ 2025-01-16
- Sukhbaatar et al Augmenting self-attention with persistent memory. ✅ 2025-01-16
- Ainsli Etc: Encoding long and structured data in transformers. ✅ 2025-01-16
- Choromanski Masked language modEling for proteins via linearly scalable long-context transformers. ✅ 2025-01-16
- Peng Random feature attention ✅ 2025-01-16
- Dai Transformer-xl: Attentive language models beyond a fixed-length context. ✅ 2025-01-16
- Jaegle Perceiver io: A general architecture for structured inputs & outputs. ✅ 2025-01-16
- Dai et al Funnel-transformer: Filtering out Sequential redundancy for efficient language processing. ✅ 2025-01-16
- Liu Swin transformer: Hierarchical vision transformer using shifted windows. ✅ 2025-01-16
- Tay Charformer: Fast character Transformers via gradient-based subword tokenization. ✅ 2025-01-16
- Xiong Simple local attentions remain competitive for long Context tasks. ✅ 2025-01-16
- Fedus Switch transformers: Scaling to trillion Parameter models with simple and efficient sparsity. ✅ 2025-01-16
- Zoph  Designing effective sparse expert models. ✅ 2025-01-16
- Gshard: Scaling giant models With conditional computation and automatic sharding. ✅ 2025-01-16
- Lample Large memory layers with product keys ✅ 2025-01-16

文献一句话总结1
- 门控机制
- 超越attention 结构
	-  Mamba 算法和 Mamba 2 算法状态空间
		- 基于结构化状态空间序列模型（S4）中使用的序列变换，简化了常用的 SSM 块，形成了简化的 SSM 架构
		- 结构化状态空间对偶（SSD）框架，理论上将 SSM 和不同形式的注意力连接到了一起
- 核函数
- 硬件高效

## 线性注意力发展方向总结

随着深度学习模型尤其是基于注意力机制的模型在处理大规模数据时取得的显著成果，线性注意力机制逐渐成为提升计算效率和模型可扩展性的一个重要研究方向。以下是线性注意力机制未来发展的几个主要方向：

### 1. **门控机制**

门控机制（Gating Mechanisms）已经在多个深度学习架构中证明了其有效性，特别是在序列建模和注意力计算中。在传统的自注意力机制中，计算复杂度较高，主要体现在对输入序列中每一对元素进行全连接计算时，随着序列长度的增加，计算成本呈二次方增长。门控机制通过在不同计算步骤之间动态选择性地传递信息，可以有效地控制计算流程，从而减少冗余计算。未来的线性注意力模型可能会融合门控机制，进一步提高计算效率，同时保持模型的表达能力。这种门控策略能够智能地决定哪些信息需要被关注和传递，优化注意力计算的效率，特别是在长序列任务中的应用。

### 2. **超越注意力结构**

近年来，许多研究尝试超越传统的自注意力结构，探索新的计算框架来替代或优化注意力机制。在这方面，Mamba算法及其升级版Mamba 2算法提供了值得关注的研究方向。这些算法利用了一种基于结构化状态空间序列模型（S4）的方式，简化了常用的状态空间模型（SSM）块，形成了简化的SSM架构。

- **Mamba和Mamba 2算法状态空间**：Mamba算法通过利用状态空间模型（SSM）中对序列的变换，简化了传统方法中的复杂计算。这些简化的SSM架构能够在不牺牲模型性能的情况下显著提高计算效率，为长序列建模提供了一种新的解决方案。Mamba 2则进一步提升了这一框架的灵活性和效果，通过优化状态空间的计算方式，能够在大规模数据集上实现高效的训练与推理。
    
- **结构化状态空间对偶（SSD）框架**：SSD框架通过将SSM与不同形式的注意力机制结合，提出了一种理论上的新方向。该框架通过对偶方法将状态空间模型的优势与注意力机制的灵活性相结合，使得模型在处理长序列数据时能够兼顾精度和效率。这种跨领域的结合为未来的线性注意力机制提供了新的理论支持和应用潜力。

### 3. **核函数**

核函数（Kernel Functions）在支持向量机（SVM）和高维数据处理等领域具有广泛应用。在线性注意力中，核函数被用来近似传统的点积计算，从而减少计算复杂度。核化方法将输入的注意力矩阵转化为低维空间中的函数映射，从而避免了直接进行全连接矩阵乘法的高计算成本。这使得模型能够在保持计算效率的同时，仍然捕捉到数据中的重要信息。

核函数的应用不仅限于简化计算，还能够通过引入非线性变换，增强模型的表达能力。例如，常见的高斯核、线性核和多项式核等，都能够有效地对注意力矩阵进行变换，提供更加灵活的表示方式。未来，结合核函数的线性注意力模型有望在更复杂的任务中表现出更高的效率和更强的泛化能力。

### 4. **硬件高效**

随着计算硬件的不断发展，如何让线性注意力模型能够充分利用硬件加速能力，成为提升其计算效率的一个关键方向。当前，许多深度学习模型的计算瓶颈主要出现在对大量参数和大规模数据的处理上，特别是对于长序列输入，内存和计算能力的消耗尤为严重。

为了解决这一问题，研究人员正在探索如何优化线性注意力模型，使其能够更好地适配现代硬件架构，如图形处理单元（GPU）、张量处理单元（TPU）以及定制硬件加速器等。这包括但不限于优化内存访问模式、并行化计算过程以及降低精度以减少硬件计算负担。未来，硬件高效的线性注意力模型将能够在较低的计算成本下处理更大规模的数据集，满足现实中对大规模数据处理和实时推理的需求。

## 总结

线性注意力机制的研究方向正在朝着更加高效、灵活和可扩展的方向发展。通过引入门控机制、超越传统的注意力结构、核函数近似计算以及硬件优化等策略，未来的线性注意力模型将能够在处理长序列数据时大幅度提高计算效率，同时保持较高的性能。随着这些技术的发展，线性注意力有望成为处理大规模、复杂数据的核心技术之一。

以下是您提到的各个模型的简要介绍和它们的特点：

1. **Linear Attention + Kernel + Norm**：
   - 线性注意力机制（Linear Attention）通过使用核函数（Kernel）来替代传统的注意力机制中的指数运算，简化了计算过程。这种方法通过特征映射（Feature Map）和核函数来计算注意力分数，从而避免了显式的矩阵乘法，使得计算更加高效。

2. **DeltaNet**：
   - DeltaNet将Delta Rule（一种基本的误差校正学习原则）应用于线性注意力中。它基于预测误差更新状态，而不是简单地累积键值外积。

3. **Gated RFA (Random Feature Attention)**：
   - Gated RFA是线性注意力的一种变体，通过引入门控机制来增强模型的表达能力。

4. **S4 (Structured State Space Model)**：
   - S4是一种结构化状态空间模型，它通过选择性地传播或遗忘序列长度维度上的信息来提高效率。

6. **RetNet**：
   - RetNet可能是递归网络的一种变体，但搜索结果中没有提供具体的信息。

7. **Mamba**：
   - Mamba是一种简化的端到端神经网络架构，无需关注或MLP块。它具有快速推理能力（吞吐量比Transformers高5倍），并且在序列长度上呈线性扩展。

8. **GLA (Gated Linear Attention)**：
   - GLA通过引入门控机制来增强线性注意力的能力，允许模型有选择性地传播或遗忘信息。

9. **RWKV-6**：
   - 没有找到具体的信息，RWKV-6可能指的是一种递归神经网络的变体，但没有具体的模型描述。

10. HGRN2: Gated Linear RNNs with State Expansion
    - 没有找到具体的信息，HGRN-2可能指的是一种层次图循环网络的变体，但没有具体的模型描述。

11. **mLSTM (Modified LSTM)**：
    - 没有找到具体的信息，mLSTM可能指的是对传统LSTM的某种修改，但没有具体的模型描述。

12. **Mamba-2**：
    - Mamba-2是Mamba的升级版，引入了新的SSD层，这是选择性SSM的一种特例。Mamba-2在训练效率上有显著提升，并且能够更好地利用现代加速器上的矩阵乘法单元。 它还提供了状态空间模型的显式矩阵变换形式，为理解和使用状态空间模型提供了新的方法。 

这些模型都在探索如何提高长序列处理的效率和效果，特别是在深度学习和自然语言处理领域。
